{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rana/anaconda3/envs/m2f/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coco_lvis_2017_train datset registered\n",
      "Berkeley data registered\n",
      "/home/rana/Thesis/DynaMITe/datasets/GrabCut\n",
      "davis_2017_val datset registered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rana/anaconda3/envs/m2f/lib/python3.8/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch,torchvision\n",
    "import copy\n",
    "from mask2former.utils.misc import is_dist_avail_and_initialized, nested_tensor_from_tensor_list\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# y, _= nested_tensor_from_tensor_list(x).decompose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(1,3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarising multi-instance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_root_dir = os.path.join(os.getcwd(), \"output/evaluations/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_path = _root_dir + \"sbd_multi_insts/mq_baseline_mit_b0_bs32_ep50_per_obj_02_03_2023_21_20_33__10.pickle\"\n",
    "with open(pickle_path, 'rb') as handle:\n",
    "    b= pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: sbd_multi_insts\n",
      "iou_threshold: 0.85\n",
      "NOC: 6.02\n",
      "NCI: 4.056120028223635\n",
      "NFO: 1747\n",
      "failed_images_counts: 847\n"
     ]
    }
   ],
   "source": [
    "print(f\"dataset: {b['dataset']}\")\n",
    "print(f\"iou_threshold: {b['iou_threshold']}\")\n",
    "print(f\"NOC: {b['Avg_NOC']}\")\n",
    "print(f\"NCI: {b['avg_over_total_images']}\")\n",
    "print(f\"NFO: {b['num_failed_objects']}\")\n",
    "print(f\"failed_images_counts: {b['failed_images_counts']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2008_000003_0',\n",
       " '2008_000007_1',\n",
       " '2008_000009_2',\n",
       " '2008_000027_3',\n",
       " '2008_000043_4',\n",
       " '2008_000051_5',\n",
       " '2008_000059_6',\n",
       " '2008_000067_7',\n",
       " '2008_000073_8',\n",
       " '2008_000075_9']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = list(b[\"ious_objects_per_interaction\"].keys())\n",
    "ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor(0.8549), tensor(0.9158)],\n",
       " [tensor(0.8549), tensor(0.9158)],\n",
       " [tensor(0.8549), tensor(0.9158)],\n",
       " [tensor(0.8549), tensor(0.9158)],\n",
       " [tensor(0.8549), tensor(0.9158)]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[\"ious_objects_per_interaction\"]['2008_000067_7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[True, True, True],\n",
       " [True, False, False],\n",
       " [False, False, True],\n",
       " [True, False, False],\n",
       " [False, False, True]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[\"clicked_objects_per_interaction\"]['2008_000067_7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ious = [torch.tensor(0.8549), torch.tensor(0.9158)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_times_point_smapled_false = 0\n",
    "while True:\n",
    "    if all(iou >= 0.85 for iou in ious) or num_times_point_smapled_false >= 2:\n",
    "        break\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(b[\"ious_objects_per_interaction\"].keys())\n",
    "total_iou_change = 0\n",
    "count = 0\n",
    "bins = 101\n",
    "neg_iou_change = [0]*bins\n",
    "pos_iou_change = [0]*bins\n",
    "\n",
    "neg_iou_change_bg = [0]*bins\n",
    "pos_iou_change_bg = [0]*bins\n",
    "bg_clicks = 0\n",
    "fg_clicks = 0\n",
    "for key in ids:\n",
    "    ious = np.asarray(b[\"ious_objects_per_interaction\"][key])\n",
    "    clicked = b[\"clicked_objects_per_interaction\"][key]\n",
    "    indices = [sum(i)>=1 for i in clicked]\n",
    "    clicked = np.asarray(clicked)[indices] \n",
    "    if len(indices)<len(ious):\n",
    "        indices.append(False)\n",
    "    \n",
    "    ious = ious[indices]\n",
    "    assert len(ious) == len(clicked)\n",
    "    \n",
    "    for i in range(1,len(clicked)):\n",
    "        if len(ious[i])==1:\n",
    "            continue\n",
    "        diff = (ious[i]-ious[i-1])*100\n",
    "        t = np.where(clicked[i]==True)[0][0]\n",
    "        if t== len(clicked[i])-1: #bg_click\n",
    "            bg_clicks+=1\n",
    "            for d in diff:\n",
    "                if d>0:\n",
    "                   pos_iou_change_bg[int(abs(d))]+=1\n",
    "                elif d<0:\n",
    "                   neg_iou_change_bg[int(abs(d))]+=1 \n",
    "        else:\n",
    "            # diff[t] = 0\n",
    "            fg_clicks+=1\n",
    "            for (i, d) in enumerate(diff):\n",
    "                if i!=t:\n",
    "                    if d>0:\n",
    "                        pos_iou_change[int(abs(d))]+=1\n",
    "                    elif d<0:\n",
    "                        neg_iou_change[int(abs(d))]+=1 \n",
    "\n",
    "        # # print(diff)\n",
    "        # total_iou_change += (sum(diff)*100/(len(diff)-1))\n",
    "        # count+=1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg_clicks: 26738\n",
      "\n",
      "negative iou change(>=3%) : 0\n",
      "\n",
      "positive iou change(>=3%) : 0\n",
      "\n",
      "bg_clicks: 5659\n",
      "\n",
      "negative iou change bg(>=3%) : 0\n",
      "\n",
      "positive iou change bg(>=3%) : 0\n"
     ]
    }
   ],
   "source": [
    "print(f'fg_clicks: {fg_clicks}\\n')\n",
    "print(f'negative iou change(>=3%) : {sum(neg_iou_change[3:])}\\n')\n",
    "print(f'positive iou change(>=3%) : {sum(pos_iou_change[3:])}\\n')\n",
    "\n",
    "print(f'bg_clicks: {bg_clicks}\\n')\n",
    "print(f'negative iou change bg(>=3%) : {sum(neg_iou_change_bg[3:])}\\n')\n",
    "print(f'positive iou change bg(>=3%) : {sum(pos_iou_change_bg[3:])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(neg_iou_change)\n",
    "sum(neg_iou_change[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    print(\"x_g\",x)\n",
    "    x+=1\n",
    "    print(\"x_g_\",x)\n",
    "\n",
    "def f(x):\n",
    "\n",
    "    for i in range(5):\n",
    "        g(x)\n",
    "        x+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_g 0\n",
      "x_g_ 1\n",
      "x_g 1\n",
      "x_g_ 2\n",
      "x_g 2\n",
      "x_g_ 3\n",
      "x_g 3\n",
      "x_g_ 4\n",
      "x_g 4\n",
      "x_g_ 5\n"
     ]
    }
   ],
   "source": [
    "f(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"weights/segformer/mit_b0_trans.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price = [5,3,2,10]\n",
    "# max_from_last= [0]*len(price)\n",
    "# max_from_last[-1] = price[-1]\n",
    "_max = price[-1]\n",
    "profit = 0\n",
    "for i in range(len(price)-2,-1,-1):\n",
    "    _max= max(price[i], _max)\n",
    "    profit += (_max-price[i])\n",
    "profit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \" backbone.bottom_up.patch_embed1.proj.{bias, weight}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'backbone.patch_embed1.proj.{bias, weight}'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.lstrip(' ').replace(\".bottom_up\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "a = torch.ones((1024,1024)).to(dtype=torch.float)\n",
    "sys.getsizeof(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 1024])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.projects.point_rend.point_features import (\n",
    "    get_uncertain_point_coords_with_randomness,\n",
    "    point_sample,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 1., 1., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 1., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "a = torch.zeros((10,10))\n",
    "a[0:6,0:3] = 1\n",
    "a[4:7,4:7] = 1\n",
    "# a[0:5, 7:] =1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 2.],\n",
       "        [1., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 2.],\n",
       "        [2., 0.],\n",
       "        [2., 1.],\n",
       "        [2., 2.],\n",
       "        [3., 0.],\n",
       "        [3., 1.],\n",
       "        [3., 2.],\n",
       "        [4., 0.],\n",
       "        [4., 1.],\n",
       "        [4., 2.],\n",
       "        [4., 4.],\n",
       "        [4., 5.],\n",
       "        [4., 6.],\n",
       "        [5., 0.],\n",
       "        [5., 1.],\n",
       "        [5., 2.],\n",
       "        [5., 4.],\n",
       "        [5., 5.],\n",
       "        [5., 6.],\n",
       "        [6., 4.],\n",
       "        [6., 5.],\n",
       "        [6., 6.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = torch.stack(torch.where(a), dim=1).to(torch.float)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.stack(torch.where(a), dim=1).to(torch.float)\n",
    "p[:,0]/=float(1024)\n",
    "p[:,1]/=float(1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones((1,5,3))\n",
    "x.mean(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rana/anaconda3/envs/m2f/lib/python3.8/site-packages/torch/nn/functional.py:3981: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y = point_sample(x, p.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1, 2, 0], [1, 2, 0], [1, 2, 0]],\n",
       " [[4, 5, 0], [4, 5, 0], [4, 5, 0], [4, 5, 0]],\n",
       " [[6, 6, 1], [6, 6, 1], [6, 6, 1], [6, 6, 1], [6, 6, 1]]]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[[1,2,0]]*3, [[4,5,0]]*4, [[6,6,1]]*5]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 0],\n",
       " [1, 2, 0],\n",
       " [1, 2, 0],\n",
       " [4, 5, 0],\n",
       " [4, 5, 0],\n",
       " [4, 5, 0],\n",
       " [4, 5, 0],\n",
       " [6, 6, 1],\n",
       " [6, 6, 1],\n",
       " [6, 6, 1],\n",
       " [6, 6, 1],\n",
       " [6, 6, 1]]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = copy.deepcopy(a[0])\n",
    "for t in a[1:]:\n",
    "    y.extend(t)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor(y)[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [1, 2],\n",
       "        [1, 2],\n",
       "        [4, 5],\n",
       "        [4, 5],\n",
       "        [4, 5],\n",
       "        [4, 5],\n",
       "        [6, 6],\n",
       "        [6, 6],\n",
       "        [6, 6],\n",
       "        [6, 6],\n",
       "        [6, 6]])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1, 2, 0], [1, 2, 0], [1, 2, 0]],\n",
       " [[4, 5, 0], [4, 5, 0], [4, 5, 0], [4, 5, 0]],\n",
       " [[6, 6, 1], [6, 6, 1], [6, 6, 1], [6, 6, 1], [6, 6, 1]]]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([41, 2])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = torch.zeros((len(torch.where(a)[0]),2))\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 1., 1.,  ..., 0., 0., 0.],\n",
       "          [0., 1., 1.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = F.interpolate(a.unsqueeze(0).unsqueeze(0), size=(128,128), mode=\"bilinear\", align_corners=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  2.,  3.],\n",
       "       [ 4.,  5.,  6.,  7.],\n",
       "       [ 8.,  9., 10., 11.],\n",
       "       [12., 13., 14., 15.]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import ndimage\n",
    "import numpy as np\n",
    "a = np.arange(16.).reshape((4, 4))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  4.,  8., 12.],\n",
       "       [ 1.,  5.,  9., 13.],\n",
       "       [ 2.,  6., 10., 14.],\n",
       "       [ 3.,  7., 11., 15.]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.5, 5. ])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndimage.map_coordinates(a, [[1], [0.5, 1]], order=1, mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17552/3095995108.py:1: DeprecationWarning: Please use `map_coordinates` from the `scipy.ndimage` namespace, the `scipy.ndimage.interpolation` namespace is deprecated.\n",
      "  from scipy.ndimage.interpolation import map_coordinates\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "# from mpl_toolkits.basemap import interp\n",
    "import numpy\n",
    "\n",
    "in_data = numpy.array([[ 25.89125824,  25.88840675],[ 25.90930748,  25.90640068]], dtype=numpy.float32)\n",
    "\n",
    "in_data.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.8, 7.5])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_coordinates(a, [[1.7,1.5], [1,1.5]], order=1, mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "num_labels, labels_im = cv2.connectedComponents(a.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 0, 0, 0, 2, 2, 2],\n",
       "       [1, 1, 1, 0, 0, 0, 0, 2, 2, 2],\n",
       "       [1, 1, 1, 0, 0, 0, 0, 2, 2, 2],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 2, 2, 2],\n",
       "       [0, 0, 0, 0, 3, 3, 0, 2, 2, 2],\n",
       "       [0, 0, 0, 0, 3, 3, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([72,  9, 15,  4])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(labels_im.flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_im == np.argmax(np.bincount(labels_im.flat)[1:]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mask2former.data.dataset_mappers.eval.davis17_sbd_mq_evaluation_clicks_mapper import DAVIS17SBDEvalMQClicksDatasetMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mask2former.data.datasets.register_coco_lvis import *\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "\n",
    "_root = os.getcwd()\n",
    "_root = os.path.join(_root, \"datasets/\")\n",
    "# _root = os.getenv(\"DETECTRON2_DATASETS\", \"datasets\")\n",
    "# print(_root)\n",
    "# register_all_coco_lvis_2017(_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = DatasetCatalog.get(\"coco_lvis_2017_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ious = np.array([.76,.78,.80,.82,.85,.87,.90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals = ious>=.80\n",
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2023-02-08 10:36:37.880132\n",
      "date and time = 08_02_2023_10_36_37s\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    " \n",
    "print(\"now =\", now)\n",
    "\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "print(\"date and time =\", dt_string +\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 4, 3, 2, 1, 0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(5,-1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "10 in a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 1, 1],\n",
       "        [0, 0, 0, 1, 1],\n",
       "        [1, 1, 1, 0, 1],\n",
       "        [1, 0, 1, 0, 0],\n",
       "        [1, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((2,5,5))\n",
    "torch.argmax(x,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2960,  0.8196, -1.1057, -0.7486,  0.0711, -1.1511, -0.5144, -0.5536,\n",
      "         1.3287, -0.3221], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10, dtype=torch.float32, requires_grad=True)\n",
    "print(x)\n",
    "y = x.repeat(5, 1)\n",
    "z = (y**2).sum()\n",
    "# z.backward()\n",
    "torch.autograd.backward([z], inputs=[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD([x], lr=0.01)\n",
    "print(x)        # tensor([1., 2.], requires_grad=True)\n",
    "optim.step()\n",
    "print(x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0929,  1.6599,  0.0832,  0.5620],\n",
      "        [-0.3079,  2.0893, -1.9158,  1.1371],\n",
      "        [-0.5511, -0.2064,  0.6665,  0.4380]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 4, requires_grad=True)\n",
    "print(x)\n",
    "# def test_repeat(x):\n",
    "y = x.repeat(2, 2, 2, 2)\n",
    "out = y.sum()\n",
    "out.backward()\n",
    "\n",
    "# test_repeat(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0929,  1.6599,  0.0832,  0.5620],\n",
      "        [-0.3079,  2.0893, -1.9158,  1.1371],\n",
      "        [-0.5511, -0.2064,  0.6665,  0.4380]], requires_grad=True)\n",
      "tensor([[-0.1089,  1.6439,  0.0672,  0.5460],\n",
      "        [-0.3239,  2.0733, -1.9318,  1.1211],\n",
      "        [-0.5671, -0.2224,  0.6505,  0.4220]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "optim = torch.optim.SGD([x], lr=0.001)\n",
    "print(x)        # tensor([1., 2.], requires_grad=True)\n",
    "optim.step()\n",
    "print(x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.ones((5,5),dtype=np.bool_)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 1.],\n",
       "       [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.ones((5,5))\n",
    "y[2:4,1:4] =0\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.argwhere(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "indices = random.sample(range(s.shape[0]),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 3, 16, 8, 10]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "@lru_cache(maxsize=None)\n",
    "def generate_probs(max_num_points, gamma):\n",
    "    probs = []\n",
    "    last_value = 1\n",
    "    for i in range(max_num_points):\n",
    "        probs.append(last_value)\n",
    "        last_value *= gamma\n",
    "\n",
    "    probs = np.array(probs)\n",
    "    probs /= probs.sum()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.36060726, 0.25242508, 0.17669756, 0.12368829, 0.0865818 ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_probs = generate_probs(5,gamma=0.7)\n",
    "pos_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [[1,2,0], [2,3]]\n",
    "y = []\n",
    "y.append(x[1].append(0))\n",
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "use_timestamp=False\n",
    "def gen_sineembed_for_position(pos_tensor):\n",
    "    # n_query, bs, _ = pos_tensor.size()\n",
    "    # sineembed_tensor = torch.zeros(n_query, bs, 256)\n",
    "    import math\n",
    "    scale = 2 * math.pi\n",
    "    dim_t = torch.arange(128, dtype=torch.float32, device=pos_tensor.device)\n",
    "    dim_t = 10000 ** (2 * torch.div(dim_t, 2, rounding_mode='floor') / 128)\n",
    "    x_embed = pos_tensor[:, :, 0] * scale\n",
    "    y_embed = pos_tensor[:, :, 1] * scale\n",
    "    if use_timestamp:\n",
    "        t_embed = pos_tensor[:, :, 2] * scale\n",
    "        y_embed += t_embed\n",
    "        x_embed += x_embed\n",
    "    pos_x = x_embed[:, :, None] / dim_t\n",
    "    pos_y = y_embed[:, :, None] / dim_t\n",
    "    pos_x[:, :, 0::2][torch.where(pos_x[:, :, 0::2] < 0)] = 0.0\n",
    "    pos_x[:, :, 1::2][torch.where(pos_x[:, :, 1::2] < 0)] = (0.5 * math.pi)\n",
    "    pos_y[:, :, 0::2][torch.where(pos_y[:, :, 0::2] < 0)] = 0.0\n",
    "    pos_y[:, :, 1::2][torch.where(pos_y[:, :, 1::2] < 0)] = (0.5 * math.pi)\n",
    "    pos_x = torch.stack((pos_x[:, :, 0::2].sin(), pos_x[:, :, 1::2].cos()), dim=3).flatten(2)\n",
    "    pos_y = torch.stack((pos_y[:, :, 0::2].sin(), pos_y[:, :, 1::2].cos()), dim=3).flatten(2)\n",
    "    pos = torch.cat((pos_y, pos_x), dim=2)\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tensor_coords(batched_fg_coords_list, batched_bg_coords_list, num_queries, height, width, device):\n",
    "\n",
    "    #batched_fg_coords_list: batch x (list of list of fg coords) [y,x,t]\n",
    "\n",
    "    # return\n",
    "    # points: Bs x num_queries x 3 \n",
    "    B = len(batched_fg_coords_list)\n",
    "    \n",
    "    pos_tensor = []\n",
    "    \n",
    "    for i, fg_coords_per_image in enumerate(batched_fg_coords_list):\n",
    "        coords_per_image  = []\n",
    "        for fg_coords_per_mask in fg_coords_per_image:\n",
    "            for coords in fg_coords_per_mask:\n",
    "                coords_per_image.append([coords[0]/width, coords[1]/height, coords[2]])\n",
    "        if batched_bg_coords_list[i] is not None:\n",
    "            for coords in batched_bg_coords_list[i]:\n",
    "                coords_per_image.append([coords[0]/width, coords[1]/height, coords[2]])\n",
    "        coords_per_image.extend([[-1.0,-1.0,-1.0]] * (num_queries-len(coords_per_image)))\n",
    "        pos_tensor.append(torch.tensor(coords_per_image,device=device))\n",
    "    # pos_tensor = torch.tensor(pos_tensor,device=device)\n",
    "    pos_tensor = torch.stack(pos_tensor)\n",
    "    return pos_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [y,x,t]\n",
    "batched_fg_coords_list = [[[[2,3,0],[3,5,0]], [[5,6,0], [6,7,0]]], [[[2,3,0],[3,5,0]], [[5,6,0], [6,7,0]]]]\n",
    "batched_bg_coords_list = [[[6,7,0]],None]\n",
    "num_queries = 11\n",
    "height = width = 50\n",
    "device = 'cpu'\n",
    "\n",
    "pos_tensor = get_pos_tensor_coords(batched_fg_coords_list, batched_bg_coords_list, num_queries, height, width, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 11, 3])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0400,  0.0600,  0.0000],\n",
       "         [ 0.0600,  0.1000,  0.0000],\n",
       "         [ 0.1000,  0.1200,  0.0000],\n",
       "         [ 0.1200,  0.1400,  0.0000],\n",
       "         [ 0.1200,  0.1400,  0.0000],\n",
       "         [-1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000]],\n",
       "\n",
       "        [[ 0.0400,  0.0600,  0.0000],\n",
       "         [ 0.0600,  0.1000,  0.0000],\n",
       "         [ 0.1000,  0.1200,  0.0000],\n",
       "         [ 0.1200,  0.1400,  0.0000],\n",
       "         [-1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000]]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 2 * math.pi\n",
    "dim_t = torch.arange(128, dtype=torch.float32, device=pos_tensor.device)\n",
    "dim_t = 10000 ** (2 * torch.div(dim_t, 2, rounding_mode='floor') / 128)\n",
    "x_embed = pos_tensor[:, :, 0] * scale\n",
    "y_embed = pos_tensor[:, :, 1] * scale\n",
    "# if use_timestamp:\n",
    "#     t_embed = pos_tensor[:, :, 2] * scale\n",
    "#     y_embed += t_embed\n",
    "#     x_embed += x_embed\n",
    "pos_x = x_embed[:, :, None] / dim_t\n",
    "pos_y = y_embed[:, :, None] / dim_t\n",
    "pos_x = torch.stack((pos_x[:, :, 0::2].sin(), pos_x[:, :, 1::2].cos()), dim=3).flatten(2)\n",
    "pos_y = torch.stack((pos_y[:, :, 0::2].sin(), pos_y[:, :, 1::2].cos()), dim=3).flatten(2)\n",
    "pos = torch.cat((pos_y, pos_x), dim=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 11, 128])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_x = x_embed[:, :, None] / dim_t\n",
    "pos_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_x[:, :, 0::2][torch.where(pos_x[:, :, 0::2] < 0)] = 0.0\n",
    "pos_x[:, :, 1::2][torch.where(pos_x[:, :, 1::2] < 0)] = math.pi * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 9.6858e-01,  9.7641e-01,  9.8229e-01,  ...,  1.0000e+00,\n",
       "           1.0000e+00,  1.0000e+00],\n",
       "         [ 9.2978e-01,  9.4718e-01,  9.6030e-01,  ...,  1.0000e+00,\n",
       "           1.0000e+00,  1.0000e+00],\n",
       "         [ 8.0902e-01,  8.5559e-01,  8.9104e-01,  ...,  1.0000e+00,\n",
       "           1.0000e+00,  1.0000e+00],\n",
       "         ...,\n",
       "         [-4.3711e-08, -4.3711e-08, -4.3711e-08,  ..., -4.3711e-08,\n",
       "          -4.3711e-08, -4.3711e-08],\n",
       "         [-4.3711e-08, -4.3711e-08, -4.3711e-08,  ..., -4.3711e-08,\n",
       "          -4.3711e-08, -4.3711e-08],\n",
       "         [-4.3711e-08, -4.3711e-08, -4.3711e-08,  ..., -4.3711e-08,\n",
       "          -4.3711e-08, -4.3711e-08]],\n",
       "\n",
       "        [[ 9.6858e-01,  9.7641e-01,  9.8229e-01,  ...,  1.0000e+00,\n",
       "           1.0000e+00,  1.0000e+00],\n",
       "         [ 9.2978e-01,  9.4718e-01,  9.6030e-01,  ...,  1.0000e+00,\n",
       "           1.0000e+00,  1.0000e+00],\n",
       "         [ 8.0902e-01,  8.5559e-01,  8.9104e-01,  ...,  1.0000e+00,\n",
       "           1.0000e+00,  1.0000e+00],\n",
       "         ...,\n",
       "         [-4.3711e-08, -4.3711e-08, -4.3711e-08,  ..., -4.3711e-08,\n",
       "          -4.3711e-08, -4.3711e-08],\n",
       "         [-4.3711e-08, -4.3711e-08, -4.3711e-08,  ..., -4.3711e-08,\n",
       "          -4.3711e-08, -4.3711e-08],\n",
       "         [-4.3711e-08, -4.3711e-08, -4.3711e-08,  ..., -4.3711e-08,\n",
       "          -4.3711e-08, -4.3711e-08]]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_x[:, :, 1::2].cos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_t = gen_sineembed_for_position(pos_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.6812e-01,  9.2978e-01,  3.2069e-01,  ...,  1.0000e+00,\n",
       "           2.9023e-05,  1.0000e+00],\n",
       "         [ 5.8779e-01,  8.0902e-01,  5.1765e-01,  ...,  1.0000e+00,\n",
       "           4.3534e-05,  1.0000e+00],\n",
       "         [ 6.8455e-01,  7.2897e-01,  6.0751e-01,  ...,  1.0000e+00,\n",
       "           7.2557e-05,  1.0000e+00],\n",
       "         ...,\n",
       "         [ 0.0000e+00, -4.3711e-08,  0.0000e+00,  ..., -4.3711e-08,\n",
       "           0.0000e+00, -4.3711e-08],\n",
       "         [ 0.0000e+00, -4.3711e-08,  0.0000e+00,  ..., -4.3711e-08,\n",
       "           0.0000e+00, -4.3711e-08],\n",
       "         [ 0.0000e+00, -4.3711e-08,  0.0000e+00,  ..., -4.3711e-08,\n",
       "           0.0000e+00, -4.3711e-08]],\n",
       "\n",
       "        [[ 3.6812e-01,  9.2978e-01,  3.2069e-01,  ...,  1.0000e+00,\n",
       "           2.9023e-05,  1.0000e+00],\n",
       "         [ 5.8779e-01,  8.0902e-01,  5.1765e-01,  ...,  1.0000e+00,\n",
       "           4.3534e-05,  1.0000e+00],\n",
       "         [ 6.8455e-01,  7.2897e-01,  6.0751e-01,  ...,  1.0000e+00,\n",
       "           7.2557e-05,  1.0000e+00],\n",
       "         ...,\n",
       "         [ 0.0000e+00, -4.3711e-08,  0.0000e+00,  ..., -4.3711e-08,\n",
       "           0.0000e+00, -4.3711e-08],\n",
       "         [ 0.0000e+00, -4.3711e-08,  0.0000e+00,  ..., -4.3711e-08,\n",
       "           0.0000e+00, -4.3711e-08],\n",
       "         [ 0.0000e+00, -4.3711e-08,  0.0000e+00,  ..., -4.3711e-08,\n",
       "           0.0000e+00, -4.3711e-08]]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.123233995736766e-17"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.cos(math.pi/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 3])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [[0,0,0]]*5\n",
    "x = torch.tensor(x)\n",
    "torch.stack((x,x)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "def visualization(batched_inputs, prev_output, batched_fg_coords_list,batched_bg_coords_list,\n",
    "                  alpha_blend=0.6, num_iter = 0):\n",
    "    image = np.asarray(batched_inputs[0]['image'].permute(1,2,0))\n",
    "\n",
    "    visualizer = Visualizer(image, metadata=None)\n",
    "    if prev_output is not None:\n",
    "        pred_masks = F.resize(prev_output.pred_masks.to(dtype=torch.uint8), image.shape[:2])\n",
    "    else:\n",
    "        pred_masks = batched_inputs[0]['instances'].gt_masks\n",
    "    c = []\n",
    "    for i in range(pred_masks.shape[0]):\n",
    "        # c.append(color_map[2*(i)+2]/255.0)\n",
    "        c.append(color_map[i]/255.0)\n",
    "    # pred_masks = np.asarray(pred_masks).astype(np.bool_)\n",
    "    vis = visualizer.overlay_instances(masks = pred_masks, assigned_colors=c, alpha=alpha_blend)\n",
    "    # [Optional] prepare labels\n",
    "\n",
    "    image = vis.get_image()\n",
    "    # # Laminate your image!\n",
    "    total_colors = len(color_map)-1\n",
    "    \n",
    "    h,w = image.shape[:2]\n",
    "    for fg_coords_per_mask in batched_fg_coords_list[0]:\n",
    "        for i, coords in enumerate(fg_coords_per_mask):\n",
    "            color = np.array(color_map[total_colors-5*i-4], dtype=np.uint8)\n",
    "            if i==0:\n",
    "                image = cv2.circle(image, (int(coords[1]), int(coords[0])), 8, color, -1)\n",
    "            else:\n",
    "                image = cv2.circle(image, (int(coords[1]), int(coords[0])), 3, color, -1)\n",
    "    \n",
    "    if batched_bg_coords_list[0]:\n",
    "         for i, coords in enumerate(batched_bg_coords_list[0]):\n",
    "            color = np.array([255,0,0], dtype=np.uint8)\n",
    "            image = cv2.circle(image, (int(coords[1]), int(coords[0])), 3, color, -1)\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    # image = cv2.resize(image, (inputs[\"width\"],inputs[\"height\"]))\n",
    "    save_dir = os.path.join(\"./train_vis/\", str(batched_inputs[0]['image_id']))\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    cv2.imwrite(os.path.join(save_dir, f\"iter_{num_iter}.jpg\"), image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones((5,5),dtype=torch.bool)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.to(torch.uint8)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"datasets/lvis/coco_lvis_combined_panoptic_1half.pickle\", 'rb') as f:\n",
    "    dataset_dicts = pickle.load(f)\n",
    "\n",
    "with open(\"datasets/lvis/coco_lvis_combined_panoptic_2half.pickle\", 'rb') as f:\n",
    "    dataset_dicts1 = pickle.load(f)\n",
    "\n",
    "dataset_dicts.extend(dataset_dicts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/lvis/coco_lvis_combined_panoptic.pickle\", 'wb') as handle:\n",
    "    pickle.dump(dataset_dicts, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99354"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"output/GrabCut_points_dict_16_02_2023_14_38_32_.pickle\", 'rb') as f:\n",
    "    pt_sampled_dict = pickle.load(f)\n",
    "\n",
    "with open(\"output/GrabCut_points_dict_16_02_2023_14_39_07_.pickle\", 'rb') as f:\n",
    "    pt_sampled_dict1 = pickle.load(f)\n",
    "\n",
    "# with open(\"output/features_dicts1_GrabCut.pickle\", 'rb') as f:\n",
    "#     features_dict1 = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pt_sampled_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_sampled_dict1.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.66"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_p = 0\n",
    "for k,v in pt_sampled_dict.items():\n",
    "    sum_p+= len(v)\n",
    "sum_p/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.66"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_p = 0\n",
    "for k,v in pt_sampled_dict1.items():\n",
    "    sum_p+= len(v)\n",
    "sum_p/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in pt_sampled_dict.keys():\n",
    "    for k1 in pt_sampled_dict1.keys():\n",
    "        if k==k1 and pt_sampled_dict[k]!=pt_sampled_dict1[k1]:\n",
    "            print(k)\n",
    "            print(pt_sampled_dict1[k1])\n",
    "            print(pt_sampled_dict[k]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/GrabCut_features_dict_16_02_2023_14_51_57_.pickle\", 'rb') as f:\n",
    "    features_dict = pickle.load(f)\n",
    "\n",
    "with open(\"output/GrabCut_features_dict_16_02_2023_14_53_59_.pickle\", 'rb') as f:\n",
    "    features_dict1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "for k in features_dict.keys():\n",
    "    for k1 in features_dict1.keys():\n",
    "        if (k==k1):\n",
    "            print(torch.all(features_dict[k]['first_mask_before_resize']==features_dict1[k1]['first_mask_before_resize']))\n",
    "            # print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bool', 'grave', '326038', 'banana1', 'book', 'memorial', 'banana2', 'bush', 'doll', 'person6', '86016', 'person3', 'person2', '227092', '209070', '21077', 'scissors', 'teddy', '65019', '271008', 'flower', 'person5', '189080', 'person8', 'sheep', '388016', '69020', 'person7', 'banana3', '37073', 'person1', '124080', '153077', 'music', '106024', 'cross', 'fullmoon', 'tennis', 'elefant', 'stone1', 'llama', '208001', 'stone2', '304074', '153093', 'ceramic', '24077', 'person4', '376043', '181079'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sem_seg_head.predictor.query_embed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/rana/claix_work/DynaMITe/visual_results.ipynb Cell 24'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/rana/claix_work/DynaMITe/visual_results.ipynb#ch0000023?line=0'>1</a>\u001b[0m features_dict[\u001b[39m'\u001b[39;49m\u001b[39mbush\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39msem_seg_head.predictor.query_embed\u001b[39;49m\u001b[39m'\u001b[39;49m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sem_seg_head.predictor.query_embed'"
     ]
    }
   ],
   "source": [
    "features_dict['bush']['sem_seg_head.predictor.query_embed']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.all(features_dict['bush']['sem_seg_head.predictor.query_embed']==features_dict['bool']['sem_seg_head.predictor.query_embed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "query_embed = nn.Parameter(torch.zeros(256), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-6.4749e-01, -9.4622e-01,  9.9136e-01, -2.8006e-01, -1.1464e+00,\n",
       "         2.3682e+00, -1.9174e+00,  4.9042e-01, -1.7973e+00, -8.8009e-02,\n",
       "         2.9536e-01, -1.1632e+00,  7.8763e-01,  9.7209e-02, -4.6051e-01,\n",
       "        -1.3793e+00, -1.3135e+00, -8.6194e-01, -2.2053e+00,  1.6569e+00,\n",
       "         5.3033e-01,  2.0219e+00,  1.4238e+00, -2.7438e-01, -2.9316e-01,\n",
       "         5.7328e-01, -4.8293e-01,  4.7248e-01, -4.7783e-01, -9.5518e-01,\n",
       "         3.4036e-02,  6.3088e-01, -1.7732e-03,  3.7992e-01, -1.1083e+00,\n",
       "        -2.0908e+00, -5.4316e-01,  3.8153e-01, -2.4660e+00, -7.5894e-01,\n",
       "         1.7062e-01, -6.1475e-01,  1.0703e+00,  3.5325e-01,  2.6061e-01,\n",
       "        -3.5651e-01,  4.8227e-01,  7.7621e-01,  9.3742e-01, -1.3908e+00,\n",
       "         1.1140e+00,  7.4399e-01, -4.4368e-01,  1.0445e+00, -1.4251e+00,\n",
       "         1.7776e-01, -5.6139e-01,  1.3463e+00,  1.2909e+00,  1.0224e+00,\n",
       "         1.0126e-02, -3.6532e-01,  3.0054e-01, -1.0427e+00, -2.5550e+00,\n",
       "         1.3792e+00, -3.0180e+00, -9.9011e-01, -5.7959e-01,  1.0527e+00,\n",
       "         3.8878e-02, -1.0803e-01, -5.9967e-01,  4.4868e-02,  1.1858e+00,\n",
       "         2.9113e-01, -1.0857e+00, -2.0949e+00, -7.9009e-01,  2.5124e-01,\n",
       "         7.6897e-01, -9.9976e-01,  1.0557e+00,  3.8981e-01,  2.1673e-01,\n",
       "         9.7767e-01,  6.8974e-02,  2.2309e+00, -1.5021e+00,  7.7536e-01,\n",
       "         3.9605e-01, -4.7302e-01, -3.6076e-02,  4.3843e-01,  1.4999e+00,\n",
       "         1.2663e+00,  1.2796e+00, -1.2030e-01, -3.6931e-01,  5.8684e-01,\n",
       "         6.7701e-01, -5.1989e-01,  5.6786e-01, -1.1889e+00, -1.0553e+00,\n",
       "         3.7751e-02, -1.9447e+00, -4.7964e-02, -2.0589e+00,  2.7592e-03,\n",
       "         5.5593e-01,  1.0265e+00,  1.3137e+00, -1.1642e+00,  1.1399e+00,\n",
       "        -1.5409e-02, -7.5154e-01,  1.5284e-02,  2.7918e+00,  4.6044e-01,\n",
       "         2.9290e-01, -1.2645e+00,  6.3408e-01, -4.7565e-02,  1.4135e+00,\n",
       "         2.4492e+00, -2.5464e+00,  6.6724e-01,  6.1609e-01, -4.9666e-01,\n",
       "        -1.7132e+00, -2.5013e-01, -1.6909e+00, -1.3032e+00,  9.4466e-01,\n",
       "        -1.2466e+00, -8.8447e-01,  7.7599e-01,  5.4487e-02, -5.9656e-01,\n",
       "         2.1227e+00, -2.1781e+00,  6.3330e-01,  7.1862e-01, -1.4283e-01,\n",
       "        -3.6911e-01,  8.7073e-01, -4.0721e-01, -1.7535e+00,  4.6955e-01,\n",
       "         1.0997e-01,  7.2473e-01,  5.2788e-01, -1.3986e+00, -8.5391e-03,\n",
       "        -6.9298e-01, -4.5402e-01, -1.2027e+00,  4.8955e-01,  7.2693e-01,\n",
       "        -5.8987e-01,  2.2842e+00, -8.5941e-01, -1.8679e-01, -6.2337e-01,\n",
       "        -1.3894e+00, -5.0438e-01, -1.1180e+00, -3.8329e-01, -1.1887e+00,\n",
       "         1.4751e+00, -3.1120e-01, -1.0112e-01,  2.0425e+00, -7.3740e-01,\n",
       "         6.8982e-02, -7.7067e-01, -6.3257e-01,  2.4467e+00, -6.6894e-01,\n",
       "         2.7258e-01,  2.7284e-01,  1.0076e+00,  1.4912e-01,  8.3674e-01,\n",
       "        -7.1490e-01,  9.0045e-01,  5.7044e-01,  3.9995e-01, -5.0511e-01,\n",
       "        -7.8044e-01,  1.2110e-01,  1.0546e+00,  7.1148e-01, -3.9047e-01,\n",
       "        -3.1581e-01,  1.2579e+00,  7.5340e-01, -1.8266e+00, -1.5195e+00,\n",
       "        -1.1493e+00, -6.0186e-01,  4.5245e-01, -6.4823e-01, -8.3427e-01,\n",
       "        -1.2665e+00,  5.3608e-01,  1.7869e-01,  2.9538e-01,  1.9509e+00,\n",
       "         2.0131e-01, -1.1664e+00, -4.7805e-01, -1.2348e-01,  3.7494e-01,\n",
       "        -1.5411e-01,  1.9286e-01,  5.3970e-01,  1.0906e+00,  1.4679e+00,\n",
       "        -1.1615e+00,  1.3955e-01,  9.5172e-01, -1.7147e-01,  1.2186e+00,\n",
       "         3.3679e+00,  5.3101e-01,  2.0939e-01,  3.9060e-01, -1.9350e+00,\n",
       "         9.4561e-01, -8.3802e-01,  1.4816e+00,  4.4207e-01,  9.6252e-02,\n",
       "        -2.0045e+00, -5.4523e-01,  3.9278e-01, -1.8980e-01, -1.1358e+00,\n",
       "         1.4776e-01, -1.1891e+00, -5.2493e-01, -1.3490e+00,  1.8121e+00,\n",
       "         4.8696e-01,  1.7120e+00, -2.2816e+00,  1.9390e+00, -4.1819e-01,\n",
       "        -4.1695e-01,  3.4997e-02,  1.3171e+00, -4.9327e-01,  6.0657e-01,\n",
       "        -5.1419e-01], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.normal_(query_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = features_dict1['bool']['sem_seg_head.predictor.query_embed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.4440e-01, -3.9581e-01,  1.4902e+00,  5.7210e-01,  1.4115e+00,\n",
       "         1.1782e+00,  1.7427e-01,  7.7683e-01,  1.3897e+00,  1.3905e-01,\n",
       "         1.0957e-01, -1.4464e-01, -1.5933e+00,  1.4587e+00, -1.1386e+00,\n",
       "         4.5302e-01,  5.3029e-01,  1.9542e-01,  1.1081e+00, -7.5916e-01,\n",
       "         9.3200e-01, -4.5756e-01, -1.4237e+00,  4.0218e-01, -9.7000e-01,\n",
       "         8.2498e-01,  4.8588e-01, -7.4774e-01, -5.4895e-01,  1.4943e+00,\n",
       "         1.6388e+00, -1.4993e+00,  9.8590e-01, -3.4655e-01, -5.1271e-01,\n",
       "         2.0943e-01, -1.9139e+00, -8.3778e-02,  6.0873e-01, -1.1971e+00,\n",
       "        -1.0011e+00,  7.8411e-01,  1.2188e+00, -6.9305e-01, -1.8274e-02,\n",
       "         7.3414e-01,  6.8789e-03,  2.3990e-02,  5.2682e-01,  7.5319e-01,\n",
       "         4.7599e-02, -4.8743e-01,  1.9112e-01, -7.5404e-01,  1.4807e+00,\n",
       "        -2.0128e+00, -1.5868e+00,  1.2303e+00,  1.5168e-01, -1.4823e+00,\n",
       "        -3.3540e-01, -5.1541e-01,  1.8677e-01, -1.3345e+00,  1.0651e+00,\n",
       "         1.5484e-01,  2.3708e-01,  9.4255e-01, -8.8311e-01,  1.4929e+00,\n",
       "         1.4355e+00, -1.2682e+00,  1.0203e+00,  1.3870e-02,  1.4823e+00,\n",
       "         7.7098e-01,  9.7230e-01,  3.8899e-01, -5.8867e-02,  2.5647e-01,\n",
       "        -4.6752e-01,  2.6700e-01,  9.0024e-01, -7.0883e-01,  3.8818e-01,\n",
       "         1.2056e+00,  1.5854e-01, -9.8617e-01,  1.7885e+00, -1.7943e+00,\n",
       "         1.2156e-01, -3.6817e-01,  2.4244e-01, -7.7437e-01,  1.3706e+00,\n",
       "        -1.4406e+00, -9.5404e-01, -2.5857e+00,  6.5134e-02, -1.3723e+00,\n",
       "        -1.3688e+00,  1.4804e+00,  2.0009e+00, -2.3979e-04,  8.2576e-01,\n",
       "         8.5650e-03,  1.3519e+00, -8.6264e-02, -7.5535e-01, -7.5569e-01,\n",
       "        -6.3938e-01,  2.5007e+00,  5.1795e-01,  3.5287e-01,  4.9800e-01,\n",
       "         5.9263e-01,  1.1400e+00, -4.4381e-01, -1.5946e+00,  6.9854e-02,\n",
       "        -4.1151e-01, -1.6355e+00, -1.3183e+00, -3.0206e+00, -1.1612e+00,\n",
       "         1.1963e+00, -7.8520e-01,  2.3013e+00, -9.0768e-01, -3.0319e-01,\n",
       "         4.7949e-01,  1.4885e+00, -3.2447e-01, -2.9942e-01, -6.9012e-02,\n",
       "        -1.1887e+00,  2.2945e+00,  2.1841e-01, -1.4820e+00, -7.2050e-01,\n",
       "        -2.5777e+00, -6.6547e-01, -8.5194e-02, -9.4252e-01, -3.3302e-01,\n",
       "         2.3797e+00, -1.2646e+00, -1.1028e+00, -1.3858e+00, -1.1228e-01,\n",
       "        -9.0071e-01,  9.3493e-01, -7.5831e-01,  4.6151e-01,  1.1208e+00,\n",
       "         6.7885e-01,  2.1138e-01,  1.3974e+00, -1.0667e+00,  1.0704e+00,\n",
       "         3.1742e-01, -7.2320e-02,  3.2800e-01,  8.7610e-01, -3.7569e-01,\n",
       "        -1.4012e+00, -1.0194e+00,  2.3109e-01,  3.9590e-01, -7.8975e-01,\n",
       "         1.9139e+00, -1.3154e-01, -5.8103e-01,  5.6205e-02, -3.2848e-01,\n",
       "        -9.1075e-01,  1.3370e+00, -6.4396e-01,  8.6809e-01, -4.7063e-01,\n",
       "        -9.4010e-01, -1.4561e-01,  9.8667e-02,  1.0744e+00,  9.5792e-01,\n",
       "         4.3059e-01,  1.8471e+00, -1.2264e+00, -2.4309e-01,  6.9404e-01,\n",
       "         3.6977e-01, -1.7577e+00, -3.3670e-02,  1.4564e-01,  1.0042e+00,\n",
       "         1.9548e-01,  7.1057e-01, -8.9447e-01, -1.0284e-01, -1.2298e+00,\n",
       "        -1.1382e-01,  1.2467e+00, -6.5633e-01,  2.1445e-01,  5.3322e-01,\n",
       "        -5.8614e-01,  1.0801e+00, -6.1095e-02, -5.6686e-01, -1.7162e+00,\n",
       "         1.6804e-01, -1.4027e-01, -4.0935e-02, -9.0276e-01,  1.1298e+00,\n",
       "         2.4249e-01, -4.4163e-03,  5.5411e-01,  4.3701e-01, -1.1128e+00,\n",
       "        -2.8587e-01,  2.6027e-01, -1.6711e+00,  3.0665e-01,  1.1804e+00,\n",
       "         3.6873e-01,  1.9183e+00,  1.3402e+00, -3.7538e-01,  1.3554e+00,\n",
       "        -1.1683e+00,  7.8752e-01,  1.3382e-01, -8.4096e-02, -1.5031e+00,\n",
       "         1.2738e-01,  1.5781e-01,  9.2643e-01,  2.8784e-01, -1.0348e+00,\n",
       "         4.0055e-01, -5.1450e-01,  1.2278e+00,  2.3003e-01, -5.0176e-01,\n",
       "        -7.2641e-01, -1.1508e+00,  2.8688e-01, -8.6050e-01, -1.0303e-01,\n",
       "         7.5071e-02, -4.6012e-01, -1.3635e-01, -8.2944e-01, -9.8331e-01,\n",
       "        -3.6355e+00], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/rana/claix_work/DynaMITe/visual_results.ipynb Cell 29\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bvision-veltins/home/rana/claix_work/DynaMITe/visual_results.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m nn\u001b[39m.\u001b[39minit\u001b[39m.\u001b[39mnormal_(q)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "nn.init.normal_(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.where(features_dict['bush']['first_mask_before_resize']!=features_dict1['bush']['first_mask_before_resize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rana/anaconda3/envs/m2f/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GaussianModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GaussianModel, self).__init__()\n",
    "\n",
    "        self.register_parameter('mean', nn.Parameter(torch.zeros(1),True))\n",
    "        \n",
    "        # self.pdf = torch.distributions.Normal(self.mean,\n",
    "                                            #   torch.tensor([1.0]))\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.normal_(self.mean)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pdf = torch.distributions.Normal(self.mean,\n",
    "                                              torch.tensor([1.0]))\n",
    "        return -pdf.log_prob(x)\n",
    "\n",
    "model = GaussianModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean :  0.7252676486968994  - Negative Loglikelihood :  3.51652193069458\n",
      "mean :  0.7298170924186707  - Negative Loglikelihood :  3.5061421394348145\n",
      "mean :  0.734357476234436  - Negative Loglikelihood :  3.4958038330078125\n",
      "mean :  0.7388887405395508  - Negative Loglikelihood :  3.485507011413574\n",
      "mean :  0.7434109449386597  - Negative Loglikelihood :  3.475250720977783\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.002)\n",
    "for _ in range(5):\n",
    "  optimizer.zero_grad()\n",
    "  nll = model(torch.tensor([3.0], requires_grad=True))\n",
    "  nll.backward()\n",
    "  optimizer.step()\n",
    "  print('mean : ', model.mean.item(),\n",
    "                 ' - Negative Loglikelihood : ', nll.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, 'gauss.pth')\n",
    "torch.save(model.state_dict(), 'gauss.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('gauss.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['mean'])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('mean', tensor([-0.0619]))])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('gauss.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('mean', tensor([0.7434]))])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from iterative_train_net import Trainer\n",
    "from mask2former import COCOMultiInstStuffMultiQueriesClicksDatasetMapper, add_maskformer2_config, COCOLVISMultiInstMQClicksDatasetMapper\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.projects.deeplab import add_deeplab_config\n",
    "from mask2former.utils.equal_num_instances_batch import build_detection_train_loader_equal\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "\n",
    "cfg = get_cfg()\n",
    "add_deeplab_config(cfg)\n",
    "add_maskformer2_config(cfg)\n",
    "# cfg.merge_from_file(\"mycfg.yaml\")\n",
    "# /home/rana/claix_work/DynaMITe/configs/coco_lvis/resnet/multi_queries_stufff_clicks_R50_bs32_ep50.yaml\n",
    "cfg.merge_from_file(\"output/mq_coco_swin_tiny_bs32_ep50/config.yaml\")\n",
    "\n",
    "model_path = \"output/single_inst_mq_per_obj_coco_swin_tiny_bs32_ep50/model_final.pth\"\n",
    "model = Trainer.build_model(cfg)\n",
    "\n",
    "# DetectionCheckpointer(model, save_dir=\"output/\").resume_or_load(\n",
    "#             model_path, resume=False\n",
    "#         )\n",
    "\n",
    "# model = model['model']\n",
    "# model.eval()\n",
    "model.load_state_dict(torch.load(cfg.MODEL.WEIGHTS)[\"model\"])\n",
    "# model = torch.load(cfg.MODEL.WEIGHTS)['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0491,  0.1466, -0.5566, -0.8277,  0.0337, -0.4834,  0.6618, -0.2049,\n",
       "        -0.0588,  0.2471,  0.4428,  0.5744,  0.2546,  0.5237,  1.0650,  1.2559,\n",
       "        -0.5063, -0.7054, -0.5026,  0.3244,  0.3879,  0.5603, -0.3371, -0.9213,\n",
       "         1.0746,  0.7930, -0.1447, -0.2350, -0.5455, -0.3754, -0.5877,  0.8867,\n",
       "        -0.2563,  0.5760,  0.0996, -0.7674,  0.7588,  0.2343, -0.5574, -0.9866,\n",
       "        -0.7541,  0.3442,  0.3891, -0.1894,  0.2952,  0.0716, -0.2876, -0.3046,\n",
       "         0.4605, -0.6726,  0.1304, -0.2660,  0.0327,  0.4801,  0.3427,  1.3805,\n",
       "         0.5639, -0.4341,  0.7380,  0.4367, -0.4625, -0.5704, -0.1120, -0.1216,\n",
       "         0.0056, -0.2250, -0.6623, -0.0103, -0.4091,  0.3360, -0.1978,  0.8804,\n",
       "         0.1445, -0.0618,  0.5945,  0.0445, -0.9692, -0.4574, -1.4822, -0.0275,\n",
       "        -1.5009, -0.3918, -0.6106,  1.2462,  0.3436,  0.2367, -0.8787, -0.2510,\n",
       "         0.2018, -0.1357,  0.1206,  1.1781, -0.1904, -0.2060, -0.2927, -0.1783,\n",
       "        -0.8082,  0.3542, -0.7496, -0.5594, -0.5036,  0.2973,  0.9794, -0.7616,\n",
       "        -0.9670, -0.3619,  0.1316, -0.1408,  0.0124, -0.3508,  0.3076, -0.7279,\n",
       "        -0.5960, -0.0512, -0.2206,  0.0231,  0.5908,  0.0771,  0.5641,  0.0025,\n",
       "        -0.1892, -0.1062, -1.5548,  0.0180,  1.0107, -0.5504,  0.0212, -0.3012,\n",
       "        -0.2961, -0.6698,  0.1198, -0.3376,  0.0928, -0.1746, -0.3822,  0.4136,\n",
       "        -0.5350, -0.5528,  1.3145, -0.1414, -0.2508, -0.8146, -1.7619,  0.0855,\n",
       "        -1.7505, -0.5249,  0.0593,  0.2707,  0.7366, -0.3458,  0.4238, -1.0587,\n",
       "         0.8384,  0.1506,  0.1359, -0.2754, -0.3174,  0.1458,  0.9129, -0.0356,\n",
       "        -0.3392, -0.5592, -0.1120,  1.1771,  0.1355, -0.4128, -0.2637,  0.0935,\n",
       "        -0.2876, -0.1052, -0.9104, -0.0018,  0.2318, -0.2834,  0.7288,  0.1785,\n",
       "        -0.1834,  0.7977,  0.5601, -0.7843,  0.0619, -1.1479,  0.2386, -0.3135,\n",
       "        -0.2815, -0.4749,  0.4074, -0.0889, -0.5069, -0.3932, -0.3774,  0.9057,\n",
       "         0.3874, -1.1192,  0.2161, -0.1715, -1.2351,  0.3351,  0.3138,  0.6554,\n",
       "        -0.0724, -0.8190, -0.7770, -0.3699, -0.3917,  0.3799,  0.1707, -0.4845,\n",
       "         0.0495, -0.8615, -0.0571, -1.3715, -1.0206, -0.5041,  0.1699,  1.4668,\n",
       "        -0.9389, -0.1412, -0.3052,  0.5333, -0.4710, -0.6022,  0.3173,  1.1631,\n",
       "        -0.8497,  0.8452, -0.7277, -0.0048,  0.4470,  0.4975, -0.0906, -0.5240,\n",
       "         0.3093,  0.1874,  1.2814,  0.3164,  0.0683,  0.5711, -0.0165, -0.1119,\n",
       "         1.0660, -0.6707,  0.2927,  0.2574,  1.1957,  1.0638, -0.0379,  0.3329,\n",
       "        -0.0135, -0.3643, -0.7451, -0.1613,  1.0831, -0.3235,  0.1457, -0.2359],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['model'][\"sem_seg_head.predictor.query_embed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_objs = 0\n",
    "num_objs = []\n",
    "for d in dataset_dicts:\n",
    "    max_objs = max(max_objs, len(d['annotations']))\n",
    "    num_objs.append(len(d['annotations']))\n",
    "# len(dataset_dicts[0]['annotations'])\n",
    "max_objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/lvis/coco_lvis_combined_panoptic.pickle\", 'wb') as handle:\n",
    "    pickle.dump(dataset_dicts, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99354"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"datasets/lvis/coco_lvis_combined_panoptic.pickle\", 'rb') as f:\n",
    "    dataset_dicts = pickle.load(f)\n",
    "\n",
    "len(dataset_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pickle file:datasets/lvis/coco_lvis_combined_panoptic.pickle\n"
     ]
    }
   ],
   "source": [
    "from mask2former import COCOMultiInstStuffMultiQueriesClicksDatasetMapper, add_maskformer2_config, COCOLVISMultiInstMQClicksDatasetMapper\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.projects.deeplab import add_deeplab_config\n",
    "from mask2former.utils.equal_num_instances_batch import build_detection_train_loader_equal\n",
    "cfg = get_cfg()\n",
    "add_deeplab_config(cfg)\n",
    "add_maskformer2_config(cfg)\n",
    "# cfg.merge_from_file(\"mycfg.yaml\")\n",
    "# /home/rana/claix_work/DynaMITe/configs/coco_lvis/resnet/multi_queries_stufff_clicks_R50_bs32_ep50.yaml\n",
    "cfg.merge_from_file(\"configs/coco_lvis/resnet/multi_queries_stufff_clicks_R50_bs32_ep50.yaml\")\n",
    "\n",
    "mapper = COCOLVISMultiInstMQClicksDatasetMapper(cfg,True)\n",
    "data_loader =  build_detection_train_loader_equal(cfg, mapper=mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mask2former.utils.vis import get_visualization\n",
    "x = batch[14]\n",
    "get_visualization(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([True,True,False])\n",
    "x =torch.stack([x,x])\n",
    "print(x.shape)\n",
    "y = torch.tensor([0.4,-0.5,1])\n",
    "torch.stack([y],0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]], requires_grad=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "x = nn.Parameter(torch.zeros(2,5), True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.9249,  0.0939, -0.0056, -0.2211,  0.0067],\n",
       "        [-0.8090,  0.0233, -0.8762, -0.4058,  0.8678]], requires_grad=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.xavier_uniform_(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.9249,  0.0939, -0.0056, -0.2211,  0.0067],\n",
       "        [-0.8090,  0.0233, -0.8762, -0.4058,  0.8678]], requires_grad=True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 1.],\n",
       "       [0., 1., 1., 1., 1.],\n",
       "       [0., 1., 1., 1., 1.],\n",
       "       [0., 1., 1., 1., 1.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.zeros((5,5))\n",
    "mask[0:4,1:5]=1\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. 1. 1.]\n",
      " [0. 1. 2. 2. 1.]\n",
      " [0. 1. 2. 2. 1.]\n",
      " [0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "1 2\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "mask = np.pad(mask, ((1, 1), (1, 1)), 'constant')\n",
    "dt = cv2.distanceTransform(mask.astype(np.uint8), cv2.DIST_L2, 0)[1:-1, 1:-1]\n",
    "print(dt)\n",
    "max_dist = np.max(dt)\n",
    "coords_y, coords_x = np.where(dt == max_dist)\n",
    "print(coords_y[0], coords_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(1243456)\n",
    "indices = random.sample(range(candidates.shape[0]),1)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.where(x['padding_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]['instances'].gt_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "image = np.asarray(x[\"image\"].permute(1,2,0))\n",
    "cv2.imshow(\"img_window\",image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.asarray(x[\"instances\"].gt_masks[0])\n",
    "cv2.imshow(\"img_window\",image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[\"instances\"].gt_masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "x=1\n",
    "while(True):\n",
    "    if x==1:\n",
    "        break\n",
    "    i+=1\n",
    "    x+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rana/anaconda3/envs/m2f/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20, 25])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.zeros((10,20,20))\n",
    "b = torch.zeros((10,20,5))\n",
    "torch.cat((a,b),dim=2).shape                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5] [6, 6]\n"
     ]
    }
   ],
   "source": [
    "def fun(x,y):\n",
    "    x.append(5)\n",
    "    y.append(6)\n",
    "    return x,y\n",
    "a = [4]\n",
    "b = [6]\n",
    "(a,\n",
    "b) = fun(a,b)\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,1.0,0], dtype=np.bool_)\n",
    "a.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5000, 1.0000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.5000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1.,2.], requires_grad=True)\n",
    "b = torch.tensor([2.,3.], requires_grad=True)  \n",
    "ab = torch.cat((a,b), dim=0) # ab  = tensor([1., 2., 2., 3.], grad_fn=<CatBackward>)\n",
    "z = ab**2 # z = tensor([1., 4., 4., 9.], grad_fn=<PowBackward0>)\n",
    "out = z.mean() # out = tensor(4.5000, grad_fn=<MeanBackward0>)\n",
    "out.backward()\n",
    "print(a.grad) # tensor([1.5000, 2.5000])\n",
    "b.grad # tensor([1.5000, 2.5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.evaluation import inference_on_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_path = \"all_data/evaluations/sbd_multi_insts/dynamite_swin_tiny_1024_bs64_10_max_click_th_final_updated_time_summary.pickle\"\n",
    "\n",
    "with open(file_path, 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "b.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = ['dataset', 'model', 'iou_threshold', 'failed_images_counts', 'avg_over_total_images', 'Avg_NOC', 'Avg_IOU', 'num_failed_objects']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.48"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b['Avg_NOC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.334966748337417"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6671/2857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = b['time_per_image_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14331715965665856"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(times)/len(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6641665849995062"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times = b['time_per_image_annotation']\n",
    "sum(times)/len(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.038640099109417216"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times = b['time_per_intreaction_tranformer_decoder']\n",
    "sum(times)/len(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5366592"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.038*2.33*4.48 + 0.14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "x = PrettyTable()\n",
    "x.field_names = [\"num interactions\", \"NCI\", \"NOC\", \"NFO\", \"NFI\"]\n",
    "for i in range(6):\n",
    "    file_path = f\"all_data/evaluations/sbd_multi_insts/ablation_iter_{i}_10_max_click_th_final_updated_time_summary.pickle\"\n",
    "\n",
    "    with open(file_path, 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "    \n",
    "    x.add_row([i, np.round(b['avg_over_total_images'],2), b['Avg_NOC'], b['num_failed_objects'], b['failed_images_counts']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+------+------+-----+\n",
      "| num interactions | NCI  | NOC  | NFO  | NFI |\n",
      "+------------------+------+------+------+-----+\n",
      "|        0         | 3.94 | 5.24 | 2024 | 928 |\n",
      "|        1         | 3.87 | 5.16 | 1854 | 884 |\n",
      "|        2         | 3.67 | 4.9  | 1682 | 825 |\n",
      "|        3         | 3.63 | 4.8  | 1566 | 778 |\n",
      "|        4         | 3.51 | 4.69 | 1566 | 776 |\n",
      "|        5         | 3.58 | 4.76 | 1576 | 784 |\n",
      "+------------------+------+------+------+-----+\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_palette(num_cls):\n",
    "    palette = np.zeros(3 * num_cls, dtype=np.int32)\n",
    "\n",
    "    for j in range(0, num_cls):\n",
    "        lab = j\n",
    "        i = 0\n",
    "\n",
    "        while lab > 0:\n",
    "            palette[j*3 + 0] |= (((lab >> 0) & 1) << (7-i))\n",
    "            palette[j*3 + 1] |= (((lab >> 1) & 1) << (7-i))\n",
    "            palette[j*3 + 2] |= (((lab >> 2) & 1) << (7-i))\n",
    "            i = i + 1\n",
    "            lab >>= 3\n",
    "\n",
    "    return palette.reshape((-1, 3))\n",
    "c = get_palette(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mask2former.data.scribble.tamed_robot import TamedRobot\n",
    "t=TamedRobot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 10]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= []\n",
    "a.insert(0,5)\n",
    "a.append(None)\n",
    "a[-1] = 10\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "areas = b['failed_objects_areas']\n",
    "bin_size=2\n",
    "bins  = [sum(areas[i:i+bin_size]) for i in range(0,len(areas)-bin_size, bin_size)] \n",
    "print(len(areas))\n",
    "while(bins[-1] ==0):\n",
    "    bins.pop()\n",
    "# print(bins)\n",
    "# sns.distplot(np.arange(len(bins)), \n",
    "#     hist_kws={\n",
    "#         \"weights\": bins\n",
    "#     },\n",
    "# )\n",
    "\n",
    "import copy\n",
    "t = sum(bins[25:])\n",
    "bins1 = copy.deepcopy(bins[:25])\n",
    "bins1 = bins1.append(t)\n",
    "x = bins[:25]\n",
    "x.append(sum(bins[25:]))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "# Bring some raw data.\n",
    "frequencies = x\n",
    "# In my original code I create a series and run on that,\n",
    "# so for consistency I create a series from the list.\n",
    "freq_series = pd.Series(frequencies)\n",
    "\n",
    "x_labels = [f\"{i+1}\" for i in range(len(x)-1)]\n",
    "x_labels.append(\">25\")\n",
    "\n",
    "# Plot the figure.\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax = freq_series.plot(kind=\"bar\")\n",
    "# ax.set_title(\"Failed Object Areas Distribution\")\n",
    "\n",
    "ax.set_xlabel(\"% of Image Covered by Object\")\n",
    "ax.set_ylabel(\"#Failed Objects\")\n",
    "ax.set_xticklabels(x_labels)\n",
    "ax.set_ylim(0,825)\n",
    "ax.set_xlim(-1,26)\n",
    "rects = ax.patches\n",
    "\n",
    "# Make some labels.\n",
    "labels = [f\"{x[i]}\" for i in range(len(rects))]\n",
    "\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(\n",
    "        rect.get_x() + rect.get_width() / 2, height + 5, label, ha=\"center\", va=\"bottom\"\n",
    "    )\n",
    "plt.savefig(\"a.jpg\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_with_sizes(x):\n",
    "    obj_sizes = np.bincount(x.flatten())\n",
    "    labels = np.nonzero(obj_sizes)[0].tolist()\n",
    "    labels = [x for x in labels if x != 0]\n",
    "    return labels, obj_sizes[labels].tolist()\n",
    "def get_palette(num_cls):\n",
    "    palette = np.zeros(3 * num_cls, dtype=np.int32)\n",
    "\n",
    "    for j in range(0, num_cls):\n",
    "        lab = j\n",
    "        i = 0\n",
    "\n",
    "        while lab > 0:\n",
    "            palette[j*3 + 0] |= (((lab >> 0) & 1) << (7-i))\n",
    "            palette[j*3 + 1] |= (((lab >> 1) & 1) << (7-i))\n",
    "            palette[j*3 + 2] |= (((lab >> 2) & 1) << (7-i))\n",
    "            i = i + 1\n",
    "            lab >>= 3\n",
    "\n",
    "    return palette.reshape((-1, 3))\n",
    "color_map = get_palette(80)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(375, 500, 3)\n",
      "[0 1 2 3 4 5 6 7]\n",
      "[1, 2, 3, 4, 5, 6, 7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 375, 500])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from detectron2.utils.colormap import colormap\n",
    "# color_map = colormap(rgb=True, maximum=1)\n",
    "\n",
    "sbd_path = \"datasets/sbd/dataset/\"\n",
    "# print(os.path.exists(sbd_path + \"img/2008_000051.jpg\"))\n",
    "image_id = \"2008_000383\"\n",
    "image = cv2.imread(sbd_path + f\"img/{image_id}.jpg\")\n",
    "print(image.shape)\n",
    "# cv2.imshow(\"image\", image)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "from scipy.io import loadmat\n",
    "instances_mask = loadmat(str(sbd_path + f\"inst/{image_id}.mat\"))['GTinst'][0][0][0].astype(np.int32)\n",
    "labels, _ = get_labels_with_sizes(instances_mask)\n",
    "print(np.unique(instances_mask))\n",
    "masks = []\n",
    "import copy\n",
    "print(labels)\n",
    "for label in labels:\n",
    "    \n",
    "    temp_masks = copy.deepcopy(instances_mask)\n",
    "    temp_masks[temp_masks != label] = 0\n",
    "    temp_masks[temp_masks > 0] = 1\n",
    "    # m = instances_mask == label\n",
    "    masks.append(np.asarray(temp_masks, dtype =np.uint8))\n",
    "masks = torch.from_numpy(np.stack(masks)).to(dtype = torch.uint8)\n",
    "masks.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import Visualizer\n",
    "image = cv2.imread(sbd_path + f\"img/{image_id}.jpg\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "visualizer = Visualizer(image, metadata=None)\n",
    "# pred_masks = F.resize(result_masks_for_vis.to(dtype=torch.uint8), image.shape[:2])\n",
    "c = []\n",
    "for i in range(masks.shape[0]):\n",
    "    c.append(color_map[i]/255.0)\n",
    "# pred_masks = np.asarray(pred_masks).astype(np.bool_)\n",
    "vis = visualizer.overlay_instances(masks = masks, assigned_colors=c, alpha=0.70)\n",
    "# [Optional] prepare labels\n",
    "\n",
    "image = vis.get_image()\n",
    "image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "cv2.imshow(\"image\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = f\"output/visual_results/{image_id}.png\"\n",
    "# im = cv2.cvtColor(image,  cv2.COLOR_RGB2BGR)\n",
    "cv2.imwrite(filename, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 854, 3)\n",
      "[0 1]\n",
      "[1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 480, 854])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "davis_path = \"datasets/DAVIS/DAVIS-2017-trainval/JPEGImages/480p/\"\n",
    "ann_path = \"datasets/DAVIS/DAVIS-2017-trainval/Annotations/480p/\"\n",
    "image_id = \"flamingo/00012\"\n",
    "image = cv2.imread(davis_path + f\"{image_id}.jpg\")\n",
    "print(image.shape)\n",
    "cv2.imshow(\"image\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "# from scipy.io import loadmat\n",
    "# instances_mask = loadmat(str(sbd_path + f\"inst/{image_id}.mat\"))['GTinst'][0][0][0].astype(np.int32)\n",
    "instances_mask = np.array(Image.open(ann_path+f\"{image_id}.png\").convert(\"P\")).astype(np.uint8)\n",
    "labels, _ = get_labels_with_sizes(instances_mask)\n",
    "print(np.unique(instances_mask))\n",
    "masks = []\n",
    "import copy\n",
    "print(labels)\n",
    "for label in labels:\n",
    "    \n",
    "    temp_masks = copy.deepcopy(instances_mask)\n",
    "    temp_masks[temp_masks != label] = 0\n",
    "    temp_masks[temp_masks > 0] = 1\n",
    "    # m = instances_mask == label\n",
    "    masks.append(np.asarray(temp_masks, dtype =np.uint8))\n",
    "masks = torch.from_numpy(np.stack(masks)).to(dtype = torch.uint8)\n",
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import Visualizer\n",
    "image = cv2.imread(davis_path + f\"{image_id}.jpg\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "visualizer = Visualizer(image, metadata=None)\n",
    "# pred_masks = F.resize(result_masks_for_vis.to(dtype=torch.uint8), image.shape[:2])\n",
    "c = []\n",
    "for i in range(masks.shape[0]):\n",
    "    c.append(color_map[2*(i)+2])\n",
    "# pred_masks = np.asarray(pred_masks).astype(np.bool_)\n",
    "vis = visualizer.overlay_instances(masks = masks, assigned_colors=c, alpha=0.65)\n",
    "# [Optional] prepare labels\n",
    "\n",
    "image = vis.get_image()\n",
    "image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "cv2.imshow(\"image\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = f\"output/visual_results/lindy-hop.png\"\n",
    "# im = cv2.cvtColor(image,  cv2.COLOR_RGB2BGR)\n",
    "cv2.imwrite(filename, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "image =  cv2.imread(\"failed_v0.png\", cv2.COLOR_BGR2RGB)\n",
    "cv2.imshow(\"image\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(501, 737, 3)\n",
      "(334, 491, 3)\n"
     ]
    }
   ],
   "source": [
    "h, w, _ = image.shape\n",
    "print(image.shape)\n",
    "h_new = int((2*h)/3)\n",
    "w_new = int((2*w)/3)\n",
    "im = cv2.resize(image,(w_new,h_new))\n",
    "print(im.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"image\", im)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# im = cv2.cvtColor(im,  cv2.COLOR_RGB2BGR)\n",
    "cv2.imwrite(\"new_failed_v0.png\", im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eec4856e9527ee4049f7304c395b2145937ffdaa8a3f2aebdeedd97affd38c5c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('m2f')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
